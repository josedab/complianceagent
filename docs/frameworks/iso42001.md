# ISO 42001 Compliance Guide

This guide covers how ComplianceAgent helps you implement ISO/IEC 42001, the international standard for AI Management Systems.

## Overview

| Attribute | Value |
|-----------|-------|
| **Full Name** | ISO/IEC 42001:2023 Artificial Intelligence Management System |
| **Jurisdiction** | International |
| **Published** | December 2023 |
| **Publisher** | ISO/IEC Joint Technical Committee 1 |
| **Type** | Certifiable management system standard |
| **Structure** | Based on ISO High-Level Structure (Annex SL) |

## Standard Structure

ISO 42001 follows the ISO management system structure:

| Clause | Title | Focus |
|--------|-------|-------|
| 4 | Context of the Organization | Scope, stakeholders, AIMS boundaries |
| 5 | Leadership | Management commitment, policy, roles |
| 6 | Planning | Risk/opportunity assessment, objectives |
| 7 | Support | Resources, competence, awareness, communication |
| 8 | Operation | AI system lifecycle, risk treatment |
| 9 | Performance Evaluation | Monitoring, audit, management review |
| 10 | Improvement | Nonconformity, corrective action |

### Annexes

| Annex | Title | Content |
|-------|-------|---------|
| A | Reference Control Objectives and Controls | 38 controls across 9 domains |
| B | Implementation Guidance | Detailed guidance for Annex A |
| C | AI System Lifecycle | Lifecycle process reference |
| D | Cross-references | Mapping to other standards |

## Annex A Control Domains

### A.2 Policies for AI

| Control | Title | Implementation |
|---------|-------|----------------|
| A.2.1 | AI policy | Documented AI governance policy |
| A.2.2 | AI system policies | System-specific policies |

### A.3 Internal Organization

| Control | Title | Implementation |
|---------|-------|----------------|
| A.3.1 | AI roles and responsibilities | Clear accountability |
| A.3.2 | Coordination between functions | Cross-functional processes |

### A.4 Resources for AI Systems

| Control | Title | Implementation |
|---------|-------|----------------|
| A.4.1 | Allocation of resources | Adequate AI resources |
| A.4.2 | AI-related competence | Skills and training |
| A.4.3 | Awareness | AI awareness programs |
| A.4.4 | Communication | Stakeholder communication |

### A.5 Assessing Impacts of AI Systems

| Control | Title | Implementation |
|---------|-------|----------------|
| A.5.1 | Impact assessment | AI impact assessment process |
| A.5.2 | Impacts on individuals | Individual impact analysis |
| A.5.3 | Societal impacts | Broader impact consideration |
| A.5.4 | Impacts documentation | Documented assessments |

### A.6 AI System Lifecycle

| Control | Title | Implementation |
|---------|-------|----------------|
| A.6.1 | AI system lifecycle | Defined lifecycle process |
| A.6.2 | AI system documentation | System documentation |
| A.6.3 | Responsible parties | Defined accountability |
| A.6.4 | Lifecycle verification | Verification activities |
| A.6.5 | Lifecycle validation | Validation activities |

### A.7 Data for AI Systems

| Control | Title | Implementation |
|---------|-------|----------------|
| A.7.1 | Data management | Data governance |
| A.7.2 | Data acquisition | Data sourcing controls |
| A.7.3 | Data quality | Quality assurance |
| A.7.4 | Data preparation | Preparation processes |

### A.8 Information for Interested Parties

| Control | Title | Implementation |
|---------|-------|----------------|
| A.8.1 | Information provision | Transparency |
| A.8.2 | Intended use | Use case documentation |
| A.8.3 | User information | User guidance |
| A.8.4 | Explainability | Explanation mechanisms |

### A.9 Use of AI Systems

| Control | Title | Implementation |
|---------|-------|----------------|
| A.9.1 | Intended use | Use within boundaries |
| A.9.2 | Human oversight | Human control mechanisms |
| A.9.3 | Monitoring | Operational monitoring |
| A.9.4 | Performance | Performance management |

### A.10 Third-Party Relationships

| Control | Title | Implementation |
|---------|-------|----------------|
| A.10.1 | Third-party relationships | Vendor management |
| A.10.2 | Provider responsibilities | Clear contractual requirements |
| A.10.3 | Customer responsibilities | Customer obligations |

## ComplianceAgent Detection

### Automatically Detected Issues

```
ISO42001-001: Missing AI policy documentation
ISO42001-002: No impact assessment for AI system
ISO42001-003: Inadequate data quality controls
ISO42001-004: Missing AI system documentation
ISO42001-005: No human oversight mechanism
ISO42001-006: Missing explainability implementation
ISO42001-007: No performance monitoring
ISO42001-008: Inadequate third-party AI assessment
ISO42001-009: Missing AI system lifecycle documentation
ISO42001-010: No awareness program for AI
```

### Example Detection

**Issue: ISO42001-002 - No impact assessment**

```python
# ❌ Non-compliant: AI system deployed without impact assessment
class RecommendationEngine:
    def __init__(self):
        self.model = load_model("recommender.pkl")
    
    def get_recommendations(self, user_id: str) -> list:
        return self.model.predict(user_id)
```

**ComplianceAgent Fix:**

```python
# ✅ Compliant: AI system with documented impact assessment
from dataclasses import dataclass
from enum import Enum
from typing import Optional

class ImpactLevel(Enum):
    MINIMAL = "minimal"
    LIMITED = "limited"
    SIGNIFICANT = "significant"
    SEVERE = "severe"

@dataclass
class AIImpactAssessment:
    """ISO 42001 A.5: Impact assessment for AI systems."""
    
    system_id: str
    system_name: str
    assessment_date: str
    assessor: str
    
    # A.5.2: Impacts on individuals
    individual_impacts: dict
    # A.5.3: Societal impacts
    societal_impacts: dict
    # Overall risk level
    risk_level: ImpactLevel
    # Mitigations
    mitigations: list
    # Review schedule
    next_review: str

# Impact assessment for recommendation engine
RECOMMENDATION_ENGINE_ASSESSMENT = AIImpactAssessment(
    system_id="rec-engine-001",
    system_name="Product Recommendation Engine",
    assessment_date="2024-01-15",
    assessor="AI Governance Team",
    
    individual_impacts={
        "autonomy": {
            "level": "limited",
            "description": "May influence purchasing decisions",
            "mitigation": "Clear labeling as AI-generated recommendations",
        },
        "privacy": {
            "level": "limited",
            "description": "Uses browsing history and purchase data",
            "mitigation": "Data minimization, user opt-out available",
        },
        "fairness": {
            "level": "minimal",
            "description": "Potential for filter bubbles",
            "mitigation": "Diversity injection in recommendations",
        },
    },
    
    societal_impacts={
        "economic": {
            "level": "minimal",
            "description": "May concentrate purchases to popular items",
            "mitigation": "Long-tail promotion mechanism",
        },
        "environmental": {
            "level": "minimal",
            "description": "Indirect impact through consumption",
            "mitigation": "Sustainable product highlighting option",
        },
    },
    
    risk_level=ImpactLevel.LIMITED,
    
    mitigations=[
        "User opt-out for personalization",
        "Transparency about recommendation criteria",
        "Regular bias audits",
        "Human oversight for high-impact categories",
    ],
    
    next_review="2024-07-15",
)

class RecommendationEngine:
    def __init__(self):
        self.model = load_model("recommender.pkl")
        self.impact_assessment = RECOMMENDATION_ENGINE_ASSESSMENT
        self._verify_assessment_current()
    
    def _verify_assessment_current(self):
        """Ensure impact assessment is up to date."""
        next_review = datetime.fromisoformat(self.impact_assessment.next_review)
        if datetime.utcnow() > next_review:
            logger.warning(
                "impact_assessment_review_due",
                system_id=self.impact_assessment.system_id,
                due_date=self.impact_assessment.next_review,
            )
    
    def get_recommendations(
        self,
        user_id: str,
        include_transparency: bool = True,
    ) -> dict:
        recommendations = self.model.predict(user_id)
        
        response = {
            "recommendations": recommendations,
            "system_id": self.impact_assessment.system_id,
        }
        
        # A.8.1: Information provision / transparency
        if include_transparency:
            response["transparency_info"] = {
                "ai_generated": True,
                "basis": "Your browsing and purchase history",
                "opt_out_url": "/settings/personalization",
            }
        
        return response
```

**Issue: ISO42001-005 - No human oversight**

```python
# ❌ Non-compliant: Automated decisions without human control
@app.post("/api/content/moderate")
async def moderate_content(content: ContentSubmission):
    result = moderation_model.classify(content.text)
    if result["category"] == "harmful":
        await remove_content(content.id)  # Automatic action
    return result
```

**ComplianceAgent Fix:**

```python
# ✅ Compliant: Human oversight for AI decisions
from enum import Enum

class OversightLevel(Enum):
    HUMAN_IN_LOOP = "human_in_loop"      # Human decides
    HUMAN_ON_LOOP = "human_on_loop"      # Human monitors, can override
    HUMAN_OVER_LOOP = "human_over_loop"  # Human can intervene later

class HumanOversightConfig:
    """ISO 42001 A.9.2: Human oversight configuration."""
    
    def __init__(self, system_id: str):
        self.system_id = system_id
        self.config = self._load_config()
    
    def get_oversight_level(
        self,
        decision_type: str,
        confidence: float,
        impact: str,
    ) -> OversightLevel:
        """Determine required human oversight level."""
        
        # High-impact decisions always require human-in-loop
        if impact == "high":
            return OversightLevel.HUMAN_IN_LOOP
        
        # Low confidence requires human review
        if confidence < 0.8:
            return OversightLevel.HUMAN_IN_LOOP
        
        # Medium impact with high confidence: human-on-loop
        if impact == "medium":
            return OversightLevel.HUMAN_ON_LOOP
        
        # Low impact: human-over-loop (can review later)
        return OversightLevel.HUMAN_OVER_LOOP

oversight_config = HumanOversightConfig("content-moderation-001")

@app.post("/api/content/moderate")
async def moderate_content(content: ContentSubmission):
    """Content moderation with human oversight per ISO 42001."""
    
    result = moderation_model.classify(content.text)
    
    # Determine human oversight requirement
    oversight_level = oversight_config.get_oversight_level(
        decision_type="content_removal",
        confidence=result["confidence"],
        impact="high" if result["category"] == "harmful" else "medium",
    )
    
    response = {
        "classification": result,
        "oversight_level": oversight_level.value,
    }
    
    if oversight_level == OversightLevel.HUMAN_IN_LOOP:
        # Create review task, don't act automatically
        review_task = await create_moderation_review(
            content_id=content.id,
            ai_classification=result,
            priority="high" if result["confidence"] > 0.9 else "normal",
        )
        response["status"] = "pending_human_review"
        response["review_task_id"] = review_task.id
        
        logger.info(
            "human_review_required",
            content_id=str(content.id),
            classification=result["category"],
            confidence=result["confidence"],
        )
        
    elif oversight_level == OversightLevel.HUMAN_ON_LOOP:
        # Act but enable easy override
        if result["category"] == "harmful":
            await flag_content(content.id, auto_action=True)
            response["action"] = "flagged"
            response["override_url"] = f"/review/content/{content.id}"
            
            # Notify moderators for monitoring
            await notify_moderators(content.id, result)
        
    else:  # HUMAN_OVER_LOOP
        # Act normally, record for later audit
        if result["category"] == "harmful":
            await flag_content(content.id, auto_action=True)
            response["action"] = "flagged"
        
        # Log for audit trail
        await record_ai_decision(
            system_id="content-moderation-001",
            decision="flag" if result["category"] == "harmful" else "allow",
            content_id=content.id,
            classification=result,
        )
    
    return response
```

**Issue: ISO42001-003 - Inadequate data quality controls**

```python
# ❌ Non-compliant: No data quality checks
def train_model(data_path: str):
    data = pd.read_csv(data_path)
    model = RandomForestClassifier()
    model.fit(data.drop("target", axis=1), data["target"])
    return model
```

**ComplianceAgent Fix:**

```python
# ✅ Compliant: Comprehensive data quality controls
from dataclasses import dataclass
from typing import Optional
import pandas as pd
import numpy as np

@dataclass
class DataQualityReport:
    """ISO 42001 A.7.3: Data quality assessment."""
    
    passed: bool
    total_records: int
    issues: list
    metrics: dict
    timestamp: str

class DataQualityChecker:
    """ISO 42001 A.7: Data quality controls for AI systems."""
    
    def __init__(self):
        self.thresholds = {
            "max_missing_ratio": 0.05,
            "max_duplicate_ratio": 0.01,
            "min_class_ratio": 0.1,
            "max_outlier_ratio": 0.05,
        }
    
    def assess_quality(
        self,
        data: pd.DataFrame,
        target_column: str,
    ) -> DataQualityReport:
        """Assess data quality for AI training."""
        
        issues = []
        metrics = {}
        
        # A.7.3.1: Completeness check
        missing_ratio = data.isnull().sum().sum() / data.size
        metrics["missing_ratio"] = missing_ratio
        if missing_ratio > self.thresholds["max_missing_ratio"]:
            issues.append({
                "type": "completeness",
                "severity": "high",
                "message": f"Missing data ratio {missing_ratio:.2%} exceeds threshold",
            })
        
        # A.7.3.2: Uniqueness check
        duplicate_ratio = data.duplicated().sum() / len(data)
        metrics["duplicate_ratio"] = duplicate_ratio
        if duplicate_ratio > self.thresholds["max_duplicate_ratio"]:
            issues.append({
                "type": "uniqueness",
                "severity": "medium",
                "message": f"Duplicate ratio {duplicate_ratio:.2%} exceeds threshold",
            })
        
        # A.7.3.3: Class balance check
        if target_column in data.columns:
            class_ratios = data[target_column].value_counts(normalize=True)
            min_class_ratio = class_ratios.min()
            metrics["min_class_ratio"] = min_class_ratio
            if min_class_ratio < self.thresholds["min_class_ratio"]:
                issues.append({
                    "type": "balance",
                    "severity": "high",
                    "message": f"Class imbalance detected, min ratio: {min_class_ratio:.2%}",
                })
        
        # A.7.3.4: Outlier detection
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        outlier_count = 0
        for col in numeric_cols:
            q1, q3 = data[col].quantile([0.25, 0.75])
            iqr = q3 - q1
            outliers = ((data[col] < q1 - 1.5*iqr) | (data[col] > q3 + 1.5*iqr)).sum()
            outlier_count += outliers
        
        outlier_ratio = outlier_count / (len(data) * len(numeric_cols))
        metrics["outlier_ratio"] = outlier_ratio
        if outlier_ratio > self.thresholds["max_outlier_ratio"]:
            issues.append({
                "type": "outliers",
                "severity": "medium",
                "message": f"Outlier ratio {outlier_ratio:.2%} exceeds threshold",
            })
        
        passed = not any(i["severity"] == "high" for i in issues)
        
        return DataQualityReport(
            passed=passed,
            total_records=len(data),
            issues=issues,
            metrics=metrics,
            timestamp=datetime.utcnow().isoformat(),
        )

data_quality_checker = DataQualityChecker()

def train_model(data_path: str, target_column: str = "target"):
    """Train model with data quality validation per ISO 42001."""
    
    data = pd.read_csv(data_path)
    
    # A.7.3: Data quality assessment
    quality_report = data_quality_checker.assess_quality(data, target_column)
    
    logger.info(
        "data_quality_assessment",
        data_path=data_path,
        records=quality_report.total_records,
        passed=quality_report.passed,
        metrics=quality_report.metrics,
    )
    
    if not quality_report.passed:
        logger.error(
            "data_quality_failed",
            issues=quality_report.issues,
        )
        raise DataQualityError(
            f"Data quality check failed: {quality_report.issues}"
        )
    
    # A.7.4: Data preparation with documentation
    X = data.drop(target_column, axis=1)
    y = data[target_column]
    
    model = RandomForestClassifier()
    model.fit(X, y)
    
    # Document training data characteristics
    training_metadata = {
        "data_path": data_path,
        "quality_report": quality_report.__dict__,
        "feature_columns": list(X.columns),
        "target_column": target_column,
        "training_timestamp": datetime.utcnow().isoformat(),
    }
    
    return model, training_metadata
```

## AI Management System Documentation

```python
# ISO 42001 Clause 7.5: Documented Information
AIMS_DOCUMENTATION = {
    "policy": {
        "ai_policy": "docs/policies/ai-policy.md",
        "ethics_policy": "docs/policies/ai-ethics.md",
        "data_governance": "docs/policies/data-governance.md",
    },
    "procedures": {
        "ai_development": "docs/procedures/ai-development-lifecycle.md",
        "impact_assessment": "docs/procedures/ai-impact-assessment.md",
        "incident_response": "docs/procedures/ai-incident-response.md",
        "change_management": "docs/procedures/ai-change-management.md",
    },
    "records": {
        "ai_inventory": "systems/ai-inventory.json",
        "impact_assessments": "assessments/",
        "training_records": "records/training/",
        "audit_logs": "logs/audit/",
        "incident_reports": "records/incidents/",
    },
    "system_documentation": {
        "template": "templates/ai-system-documentation.md",
        "systems": "systems/documentation/",
    },
}
```

## SDK Integration

```python
from complianceagent import configure, iso42001_managed, impact_assessed, data_quality

configure(regulations=["ISO42001"])

# AI system with management controls
@iso42001_managed(
    system_id="recommendation-engine",
    risk_level="limited",
    oversight="human_on_loop",
)
async def get_recommendations(user_id: str):
    return await recommendation_model.predict(user_id)

# Require impact assessment
@impact_assessed(assessment_id="ia-rec-001")
async def deploy_recommendation_model(model_path: str):
    return await deploy_model(model_path)

# Data quality validation
@data_quality(
    completeness=0.95,
    uniqueness=0.99,
    balance_threshold=0.1,
)
async def prepare_training_data(data: pd.DataFrame):
    return await preprocess(data)
```

## Compliance Dashboard

```
Dashboard → Compliance → ISO 42001

┌─────────────────────────────────────────────────────────┐
│ ISO 42001 AI Management System Status                   │
├─────────────────────────────────────────────────────────┤
│ Certification Status: Certified (expires: 2026-12-01)  │
│ Last Audit: 2024-01-15 (0 nonconformities)            │
│                                                         │
│ AI System Inventory:                                    │
│   Total Systems: 12                                    │
│   High Risk: 2    Medium Risk: 7    Low Risk: 3       │
│                                                         │
│ Control Implementation:                                 │
│   A.2 Policies:            ✅ 2/2 controls             │
│   A.3 Organization:        ✅ 2/2 controls             │
│   A.4 Resources:           ✅ 4/4 controls             │
│   A.5 Impact Assessment:   ✅ 4/4 controls             │
│   A.6 Lifecycle:           ✅ 5/5 controls             │
│   A.7 Data:                ✅ 4/4 controls             │
│   A.8 Information:         ✅ 4/4 controls             │
│   A.9 Use:                 ✅ 4/4 controls             │
│   A.10 Third-Party:        ✅ 3/3 controls             │
│                                                         │
│ Pending Actions:                                        │
│   - Impact assessment review due: 2 systems           │
│   - Training renewal due: 5 staff members             │
└─────────────────────────────────────────────────────────┘
```

## Integration with Other Standards

| Standard | Relationship |
|----------|--------------|
| **ISO 27001** | AIMS can be integrated with ISMS |
| **ISO 9001** | Quality management integration |
| **EU AI Act** | Supports regulatory compliance |
| **NIST AI RMF** | Complementary frameworks |

## Resources

- [ISO 42001:2023 Standard](https://www.iso.org/standard/81230.html)
- [ISO/IEC JTC 1/SC 42](https://www.iso.org/committee/6794475.html) - AI Standardization
- [ISO 42001 Implementation Guide](https://www.iso.org/publication/PUB100488.html)

## Related Documentation

- [NIST AI RMF Compliance](nist-ai-rmf.md)
- [EU AI Act Compliance](eu-ai-act.md)
- [ISO 27001 Compliance](iso27001.md)
